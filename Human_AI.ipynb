{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1DGfDuUDU6wW8aTLgco5ex3zYSCEh39ZL",
      "authorship_tag": "ABX9TyOere4mPPYtV9Bk+zL1/IC4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yoyodahary/ReaserchProject/blob/master/Human_AI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "PXKnEnhL09M4"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import random\n",
        "import re\n",
        "import h5py\n",
        "import math\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from tabulate import tabulate\n",
        "import os\n",
        "import time\n",
        "import psutil\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.over_sampling import ADASYN\n",
        "from multiprocessing import Process\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.model_selection import train_test_split\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "import re\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "root = \"/content/drive/MyDrive/Final_Project/\"\n",
        "datasets_path = root + 'datasets/'\n",
        "\n",
        "df = pd.read_csv(root+\"AI_Human.csv\", usecols = [\"text\",\"prompt_name\",\"source\"])\n",
        "\n",
        "df = df.sample(n=1000, random_state=42)\n",
        "\n",
        "train_df, test_df = train_test_split(df, test_size=0.25, random_state=42)\n",
        "\n",
        "folder_path = datasets_path\n",
        "if not os.path.exists(folder_path):\n",
        "    os.makedirs(folder_path)\n",
        "\n",
        "train_df.to_csv(datasets_path+'train_dataset.csv',index=False)\n",
        "train_df.to_hdf(datasets_path+'train_dataset.h5', key='train_dataset', mode='w')\n",
        "\n",
        "test_df.to_csv(datasets_path+'test_dataset.csv',index=False)\n",
        "test_df.to_hdf(datasets_path+'test_dataset.h5', key='test_dataset', mode='w')"
      ],
      "metadata": {
        "id": "Dkch1NUw1ET6"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SPECIAL_CHARCATERS_REMOVAL=r\"\\b\\w+\\b\"\n",
        "TOKEN_PATTERN=\"[^ \\n]+\"\n",
        "\n",
        "# Stop Words\n",
        "nltk.download('stopwords')\n",
        "STOP_WORDS = stopwords.words('english')\n",
        "STOP_WORDS += [word.capitalize() for word in STOP_WORDS]\n",
        "\n",
        "STOP_WORDS"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9iwEumS-QFTF",
        "outputId": "21323ec8-f676-44bb-bb52-6023fd4c5bda"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['i',\n",
              " 'me',\n",
              " 'my',\n",
              " 'myself',\n",
              " 'we',\n",
              " 'our',\n",
              " 'ours',\n",
              " 'ourselves',\n",
              " 'you',\n",
              " \"you're\",\n",
              " \"you've\",\n",
              " \"you'll\",\n",
              " \"you'd\",\n",
              " 'your',\n",
              " 'yours',\n",
              " 'yourself',\n",
              " 'yourselves',\n",
              " 'he',\n",
              " 'him',\n",
              " 'his',\n",
              " 'himself',\n",
              " 'she',\n",
              " \"she's\",\n",
              " 'her',\n",
              " 'hers',\n",
              " 'herself',\n",
              " 'it',\n",
              " \"it's\",\n",
              " 'its',\n",
              " 'itself',\n",
              " 'they',\n",
              " 'them',\n",
              " 'their',\n",
              " 'theirs',\n",
              " 'themselves',\n",
              " 'what',\n",
              " 'which',\n",
              " 'who',\n",
              " 'whom',\n",
              " 'this',\n",
              " 'that',\n",
              " \"that'll\",\n",
              " 'these',\n",
              " 'those',\n",
              " 'am',\n",
              " 'is',\n",
              " 'are',\n",
              " 'was',\n",
              " 'were',\n",
              " 'be',\n",
              " 'been',\n",
              " 'being',\n",
              " 'have',\n",
              " 'has',\n",
              " 'had',\n",
              " 'having',\n",
              " 'do',\n",
              " 'does',\n",
              " 'did',\n",
              " 'doing',\n",
              " 'a',\n",
              " 'an',\n",
              " 'the',\n",
              " 'and',\n",
              " 'but',\n",
              " 'if',\n",
              " 'or',\n",
              " 'because',\n",
              " 'as',\n",
              " 'until',\n",
              " 'while',\n",
              " 'of',\n",
              " 'at',\n",
              " 'by',\n",
              " 'for',\n",
              " 'with',\n",
              " 'about',\n",
              " 'against',\n",
              " 'between',\n",
              " 'into',\n",
              " 'through',\n",
              " 'during',\n",
              " 'before',\n",
              " 'after',\n",
              " 'above',\n",
              " 'below',\n",
              " 'to',\n",
              " 'from',\n",
              " 'up',\n",
              " 'down',\n",
              " 'in',\n",
              " 'out',\n",
              " 'on',\n",
              " 'off',\n",
              " 'over',\n",
              " 'under',\n",
              " 'again',\n",
              " 'further',\n",
              " 'then',\n",
              " 'once',\n",
              " 'here',\n",
              " 'there',\n",
              " 'when',\n",
              " 'where',\n",
              " 'why',\n",
              " 'how',\n",
              " 'all',\n",
              " 'any',\n",
              " 'both',\n",
              " 'each',\n",
              " 'few',\n",
              " 'more',\n",
              " 'most',\n",
              " 'other',\n",
              " 'some',\n",
              " 'such',\n",
              " 'no',\n",
              " 'nor',\n",
              " 'not',\n",
              " 'only',\n",
              " 'own',\n",
              " 'same',\n",
              " 'so',\n",
              " 'than',\n",
              " 'too',\n",
              " 'very',\n",
              " 's',\n",
              " 't',\n",
              " 'can',\n",
              " 'will',\n",
              " 'just',\n",
              " 'don',\n",
              " \"don't\",\n",
              " 'should',\n",
              " \"should've\",\n",
              " 'now',\n",
              " 'd',\n",
              " 'll',\n",
              " 'm',\n",
              " 'o',\n",
              " 're',\n",
              " 've',\n",
              " 'y',\n",
              " 'ain',\n",
              " 'aren',\n",
              " \"aren't\",\n",
              " 'couldn',\n",
              " \"couldn't\",\n",
              " 'didn',\n",
              " \"didn't\",\n",
              " 'doesn',\n",
              " \"doesn't\",\n",
              " 'hadn',\n",
              " \"hadn't\",\n",
              " 'hasn',\n",
              " \"hasn't\",\n",
              " 'haven',\n",
              " \"haven't\",\n",
              " 'isn',\n",
              " \"isn't\",\n",
              " 'ma',\n",
              " 'mightn',\n",
              " \"mightn't\",\n",
              " 'mustn',\n",
              " \"mustn't\",\n",
              " 'needn',\n",
              " \"needn't\",\n",
              " 'shan',\n",
              " \"shan't\",\n",
              " 'shouldn',\n",
              " \"shouldn't\",\n",
              " 'wasn',\n",
              " \"wasn't\",\n",
              " 'weren',\n",
              " \"weren't\",\n",
              " 'won',\n",
              " \"won't\",\n",
              " 'wouldn',\n",
              " \"wouldn't\",\n",
              " 'I',\n",
              " 'Me',\n",
              " 'My',\n",
              " 'Myself',\n",
              " 'We',\n",
              " 'Our',\n",
              " 'Ours',\n",
              " 'Ourselves',\n",
              " 'You',\n",
              " \"You're\",\n",
              " \"You've\",\n",
              " \"You'll\",\n",
              " \"You'd\",\n",
              " 'Your',\n",
              " 'Yours',\n",
              " 'Yourself',\n",
              " 'Yourselves',\n",
              " 'He',\n",
              " 'Him',\n",
              " 'His',\n",
              " 'Himself',\n",
              " 'She',\n",
              " \"She's\",\n",
              " 'Her',\n",
              " 'Hers',\n",
              " 'Herself',\n",
              " 'It',\n",
              " \"It's\",\n",
              " 'Its',\n",
              " 'Itself',\n",
              " 'They',\n",
              " 'Them',\n",
              " 'Their',\n",
              " 'Theirs',\n",
              " 'Themselves',\n",
              " 'What',\n",
              " 'Which',\n",
              " 'Who',\n",
              " 'Whom',\n",
              " 'This',\n",
              " 'That',\n",
              " \"That'll\",\n",
              " 'These',\n",
              " 'Those',\n",
              " 'Am',\n",
              " 'Is',\n",
              " 'Are',\n",
              " 'Was',\n",
              " 'Were',\n",
              " 'Be',\n",
              " 'Been',\n",
              " 'Being',\n",
              " 'Have',\n",
              " 'Has',\n",
              " 'Had',\n",
              " 'Having',\n",
              " 'Do',\n",
              " 'Does',\n",
              " 'Did',\n",
              " 'Doing',\n",
              " 'A',\n",
              " 'An',\n",
              " 'The',\n",
              " 'And',\n",
              " 'But',\n",
              " 'If',\n",
              " 'Or',\n",
              " 'Because',\n",
              " 'As',\n",
              " 'Until',\n",
              " 'While',\n",
              " 'Of',\n",
              " 'At',\n",
              " 'By',\n",
              " 'For',\n",
              " 'With',\n",
              " 'About',\n",
              " 'Against',\n",
              " 'Between',\n",
              " 'Into',\n",
              " 'Through',\n",
              " 'During',\n",
              " 'Before',\n",
              " 'After',\n",
              " 'Above',\n",
              " 'Below',\n",
              " 'To',\n",
              " 'From',\n",
              " 'Up',\n",
              " 'Down',\n",
              " 'In',\n",
              " 'Out',\n",
              " 'On',\n",
              " 'Off',\n",
              " 'Over',\n",
              " 'Under',\n",
              " 'Again',\n",
              " 'Further',\n",
              " 'Then',\n",
              " 'Once',\n",
              " 'Here',\n",
              " 'There',\n",
              " 'When',\n",
              " 'Where',\n",
              " 'Why',\n",
              " 'How',\n",
              " 'All',\n",
              " 'Any',\n",
              " 'Both',\n",
              " 'Each',\n",
              " 'Few',\n",
              " 'More',\n",
              " 'Most',\n",
              " 'Other',\n",
              " 'Some',\n",
              " 'Such',\n",
              " 'No',\n",
              " 'Nor',\n",
              " 'Not',\n",
              " 'Only',\n",
              " 'Own',\n",
              " 'Same',\n",
              " 'So',\n",
              " 'Than',\n",
              " 'Too',\n",
              " 'Very',\n",
              " 'S',\n",
              " 'T',\n",
              " 'Can',\n",
              " 'Will',\n",
              " 'Just',\n",
              " 'Don',\n",
              " \"Don't\",\n",
              " 'Should',\n",
              " \"Should've\",\n",
              " 'Now',\n",
              " 'D',\n",
              " 'Ll',\n",
              " 'M',\n",
              " 'O',\n",
              " 'Re',\n",
              " 'Ve',\n",
              " 'Y',\n",
              " 'Ain',\n",
              " 'Aren',\n",
              " \"Aren't\",\n",
              " 'Couldn',\n",
              " \"Couldn't\",\n",
              " 'Didn',\n",
              " \"Didn't\",\n",
              " 'Doesn',\n",
              " \"Doesn't\",\n",
              " 'Hadn',\n",
              " \"Hadn't\",\n",
              " 'Hasn',\n",
              " \"Hasn't\",\n",
              " 'Haven',\n",
              " \"Haven't\",\n",
              " 'Isn',\n",
              " \"Isn't\",\n",
              " 'Ma',\n",
              " 'Mightn',\n",
              " \"Mightn't\",\n",
              " 'Mustn',\n",
              " \"Mustn't\",\n",
              " 'Needn',\n",
              " \"Needn't\",\n",
              " 'Shan',\n",
              " \"Shan't\",\n",
              " 'Shouldn',\n",
              " \"Shouldn't\",\n",
              " 'Wasn',\n",
              " \"Wasn't\",\n",
              " 'Weren',\n",
              " \"Weren't\",\n",
              " 'Won',\n",
              " \"Won't\",\n",
              " 'Wouldn',\n",
              " \"Wouldn't\"]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Abbreviations\n",
        "ABBREVIATIONS = pd.read_excel(root+'abbreviations_eng.xls')\n",
        "ABBREVIATIONS['abbr'] = ABBREVIATIONS['abbr'].astype(str)\n",
        "ABBREVIATIONS_lowercased = ABBREVIATIONS.copy()\n",
        "ABBREVIATIONS_lowercased['abbr'] = ABBREVIATIONS_lowercased['abbr'].str.lower()\n",
        "ABBREVIATIONS_lowercased['long'] = ABBREVIATIONS_lowercased['long'].str.lower()\n",
        "ABBREVIATIONS = pd.concat([ABBREVIATIONS, ABBREVIATIONS_lowercased], ignore_index=True)\n",
        "ABBR_PATTERN = r'\\b(?:' + '|'.join(map(re.escape, ABBREVIATIONS['abbr'])) + r')\\b'\n",
        "def expand_abbreviations(text):\n",
        "    def replace(match):\n",
        "        return ABBREVIATIONS.loc[ABBREVIATIONS['abbr'] == match.group(0), 'long'].iloc[0]\n",
        "\n",
        "    return re.sub(ABBR_PATTERN, replace, text)"
      ],
      "metadata": {
        "id": "U76bq9Yq1PlZ"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def read_texts(train_or_test):\n",
        "  \"\"\"\n",
        "  reads the data from the train tweets sheet\n",
        "  :return: the data as pandas data set\n",
        "  \"\"\"\n",
        "\n",
        "  store = pd.HDFStore(datasets_path+f'{train_or_test}_dataset.h5')\n",
        "  df = store.select(f'{train_or_test}_dataset')\n",
        "  store.close()\n",
        "\n",
        "  columns_to_include = [\"text\"]\n",
        "  df = df[columns_to_include]\n",
        "  df.dropna(subset=[\"text\"], inplace=True)\n",
        "  return df\n",
        "\n",
        "\n",
        "def get_terms():\n",
        "  columns= [\"nt\", \"f\", \"tf\",\"idf\",\"tfidf\"]\n",
        "  meanings= [\"Number of different documents that the word appears in.\",\n",
        "   \"Number of appearances of the word in all documents.\",\n",
        "   \"Term frequency.\",\n",
        "   \"Inverse document frequency.\",\n",
        "   \"Term fruquency multiplied by inverse document frequency.\"]\n",
        "  return columns, meanings\n",
        "\n",
        "\n",
        "def write_tf_idf_chart(preprocessing,path = datasets_path):\n",
        "  train_text = read_texts(\"train\")\n",
        "  vectorizer = VECTORIZERS[preprocessing]\n",
        "  if \"L\" in preprocessing and \"O\" in preprocessing:\n",
        "      train_text[\"text\"] = \"LowerCode \" + train_text[\"text\"]\n",
        "  elif \"L\" in preprocessing:\n",
        "      train_text[\"text\"] = train_text[\"text\"].str.lower()\n",
        "\n",
        "  print(train_text)\n",
        "\n",
        "  sparse_matrix = vectorizer.fit_transform(train_text[\"text\"])\n",
        "  dense_matrix = sparse_matrix.toarray()\n",
        "  sig_f = sparse_matrix.sum()\n",
        "\n",
        "  print(f\"Me: sig_f is:{sig_f}\")\n",
        "\n",
        "  # Get the vocabulary from the vectorizer object\n",
        "  vocab = vectorizer.get_feature_names_out()\n",
        "  df = pd.DataFrame(columns=['word', 'nt', 'f', 'tf', 'idf', 'tf-idf'])\n",
        "\n",
        "\n",
        "  print(f\"calculating the tfidf table for {preprocessing}:\")\n",
        "  for word , index in tqdm(vectorizer.vocabulary_.items()):\n",
        "      # print(word)\n",
        "      # The column of tf-idf values of the specific word, as the rows are the tweets\n",
        "      word_column=dense_matrix[:, index]\n",
        "      # Count the number of texts the word appears in\n",
        "      nt = np.count_nonzero(word_column)\n",
        "      # Count the number of times the word is used in all tweets\n",
        "      f = np.sum(word_column)\n",
        "      # term frequency in a document compared to the number of terms in the corpus\n",
        "      tf = f / sig_f\n",
        "      # shape[0] is the number of rows, aka the number of texts\n",
        "      n = dense_matrix.shape[0]\n",
        "      # Calculate the idf value\n",
        "      idf =np. log(n / (nt + 1))\n",
        "      # Calculate the tf-idf value\n",
        "      tf_idf = tf * idf\n",
        "      # append all the data we collected so far to each word to the data frame\n",
        "      new_df = pd.DataFrame({'word': word, 'nt': nt, 'f': f, 'tf': tf, 'idf': idf, 'tf-idf': tf_idf}, index=[0])\n",
        "\n",
        "      df = pd.concat([df, new_df], ignore_index=True)\n",
        "  print(\"done!\")\n",
        "  df = df.sort_values('tf-idf', ascending=False)\n",
        "\n",
        "  df.to_csv(path+f\"tfidf_table_{preprocessing}.csv\")"
      ],
      "metadata": {
        "id": "onWW4qVl1W_F"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "NT = 1  # the minimum nuber of tweets a word in tf idf should show\n",
        "NUMBER_OF_WORDS = [1000,2000,3000,4000,5000]  # the number of the highest tfidf words\n",
        "VECTORIZERS={\n",
        "    # the tf-idf vectorizers by preprocessing method\n",
        "    # 'N' : CountVectorizer(min_df=NT,lowercase=False, token_pattern=TOKEN_PATTERN),#None, baseline\n",
        "\n",
        "    # 'S':CountVectorizer(min_df=NT,lowercase=False, token_pattern=TOKEN_PATTERN,stop_words=STOP_WORDS),#Stop word removal\n",
        "    'C':CountVectorizer(min_df=NT,lowercase=False, token_pattern=SPECIAL_CHARCATERS_REMOVAL),#special Characters removal\n",
        "    # 'O':CountVectorizer(min_df=NT,lowercase=False, token_pattern=TOKEN_PATTERN, preprocessor=expand_abbreviations),#Open abbreviations\n",
        "    # # '3gram' : CountVectorizer(min_df=NT,lowercase=False, analyzer='char', ngram_range=(3, 3)),\n",
        "    # # '4gram' : CountVectorizer(min_df=NT,lowercase=False, analyzer='char', ngram_range=(4, 4)),\n",
        "    # 'L' : CountVectorizer(min_df=NT,lowercase=True, token_pattern=TOKEN_PATTERN),# Lowercase\n",
        "\n",
        "    # # pairing preprocssing methods\n",
        "    # 'SC':CountVectorizer(min_df=NT,lowercase=False, token_pattern=SPECIAL_CHARCATERS_REMOVAL,stop_words=STOP_WORDS),\n",
        "    # 'SO':CountVectorizer(min_df=NT,lowercase=False, token_pattern=TOKEN_PATTERN,stop_words=STOP_WORDS,preprocessor=expand_abbreviations),\n",
        "    # 'SL':CountVectorizer(min_df=NT,lowercase=True, token_pattern=TOKEN_PATTERN,stop_words=STOP_WORDS),\n",
        "    # 'CO':CountVectorizer(min_df=NT,lowercase=False, token_pattern=SPECIAL_CHARCATERS_REMOVAL,preprocessor=expand_abbreviations),\n",
        "    # 'CL':CountVectorizer(min_df=NT,lowercase=True, token_pattern=SPECIAL_CHARCATERS_REMOVAL),\n",
        "    # 'OL':CountVectorizer(min_df=NT,lowercase=True, token_pattern=TOKEN_PATTERN,preprocessor=expand_abbreviations),\n",
        "\n",
        "    # # Trio preprocessing methods\n",
        "    # 'SCO':CountVectorizer(min_df=NT,lowercase=False, token_pattern=SPECIAL_CHARCATERS_REMOVAL,stop_words=STOP_WORDS,preprocessor=expand_abbreviations),\n",
        "    # 'COL':CountVectorizer(min_df=NT,lowercase=True, token_pattern=SPECIAL_CHARCATERS_REMOVAL,preprocessor=expand_abbreviations),\n",
        "    # 'SOL':CountVectorizer(min_df=NT,lowercase=True, token_pattern=TOKEN_PATTERN,stop_words=STOP_WORDS, preprocessor=expand_abbreviations),\n",
        "    # 'SCL':CountVectorizer(min_df=NT,lowercase=True, token_pattern=SPECIAL_CHARCATERS_REMOVAL,stop_words=STOP_WORDS),\n",
        "\n",
        "    # # # All preprocessing\n",
        "    # 'SCOL':CountVectorizer(min_df=NT,lowercase=True, token_pattern=SPECIAL_CHARCATERS_REMOVAL,stop_words=STOP_WORDS,preprocessor=expand_abbreviations)\n",
        "}"
      ],
      "metadata": {
        "id": "Gv_S6ms41k3n"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#itirate over each preprocessing method and writes the tfidf chart\n",
        "for preprocessing, vectorizer in VECTORIZERS.items():\n",
        "  print(\"------------------------------------------\")\n",
        "  print(f\"writing the {preprocessing} tf-idf chart:\")\n",
        "  write_tf_idf_chart(preprocessing)\n",
        "  print(\"------------------------------------------\")"
      ],
      "metadata": {
        "id": "P_M7v3Wc1nqq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "5db04750-ecb2-4709-a145-6248736e1528"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------\n",
            "writing the C tf-idf chart:\n",
            "                                                    text\n",
            "33293  Social media has become a big part of our live...\n",
            "9459   There are many dangers on venus. The atmospher...\n",
            "1784   Advantages of Limiting Car Usage\\n\\nImagine wa...\n",
            "41995  The uncontrolled proliferation of motor vehicl...\n",
            "9225   In \"The Challenge of exploring Venus,\" the aut...\n",
            "...                                                  ...\n",
            "29851  Advantages of Limiting Car Usage\\n\\nLimiting c...\n",
            "36063  [Your Name]\\n[Your Address]\\n[City, State, ZIP...\n",
            "41207  **Limiting Car Usage**\\n\\nAs a high school stu...\n",
            "28090  The advantages of limiting car usage are becom...\n",
            "7462   What if your principal decided that all of his...\n",
            "\n",
            "[750 rows x 1 columns]\n",
            "Me: sig_f is:299532\n",
            "calculating the tfidf table for C:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 11572/11572 [00:16<00:00, 691.20it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "done!\n",
            "------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "NUMBER_OF_WORDS = [5000,4000,3000,2000,1000]  # the number of the highest tfidf words\n",
        "CLASSIFY = ['label']\n",
        "# Tokenization\n",
        "SPECIAL_CHARCATERS_REMOVAL=r\"\\b\\w+\\b\"\n",
        "TOKEN_PATTERN=\"[^ \\n]+\"\n",
        "\n",
        "# Stop Words\n",
        "nltk.download('stopwords')\n",
        "STOP_WORDS = stopwords.words('english')\n",
        "\n",
        "# Abbreviations\n",
        "ABBREVIATIONS = pd.read_excel('database/abbreviations_eng.xls')\n",
        "ABBREVIATIONS['abbr'] = ABBREVIATIONS['abbr'].astype(str)\n",
        "ABBREVIATIONS_lowercased = ABBREVIATIONS.copy()\n",
        "ABBREVIATIONS_lowercased['abbr'] = ABBREVIATIONS_lowercased['abbr'].str.lower()\n",
        "ABBREVIATIONS_lowercased['long'] = ABBREVIATIONS_lowercased['long'].str.lower()\n",
        "ABBREVIATIONS = pd.concat([ABBREVIATIONS, ABBREVIATIONS_lowercased], ignore_index=True)\n",
        "ABBR_PATTERN = r'\\b(?:' + '|'.join(map(re.escape, ABBREVIATIONS['abbr'])) + r')\\b'\n",
        "def expand_abbreviations(text):\n",
        "    def replace(match):\n",
        "        return ABBREVIATIONS.loc[ABBREVIATIONS['abbr'] == match.group(0), 'long'].iloc[0]\n",
        "\n",
        "    return re.sub(ABBR_PATTERN, replace, text)"
      ],
      "metadata": {
        "id": "EpTqOuwZ15se"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}




if not os.path.exists(results_path):
    os.makedirs(results_path)
for preprocessing in preprocessings:
  folder = results_path + preprocessing
  if not os.path.exists(folder):
    os.makedirs(folder)
  for amount in NUMBER_OF_WORDS:
    amount_folder = folder + '/' + str(amount)
    if not os.path.exists(amount_folder):
      os.makedirs(amount_folder)
    for column in COLUMNS:
      column_folder = amount_folder + '/' + column
      if not os.path.exists(column_folder):
        os.makedirs(column_folder)
      for oversampler in OVERSAMPLERS:
        oversampler_folder = column_folder + '/' + oversampler
        if not os.path.exists(oversampler_folder):
          os.makedirs(oversampler_folder)
