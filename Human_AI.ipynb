{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yoyodahary/ReaserchProject/blob/master/Human_AI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "PXKnEnhL09M4"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import random\n",
        "import re\n",
        "import h5py\n",
        "import math\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from tabulate import tabulate\n",
        "import os\n",
        "import time\n",
        "import psutil\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.over_sampling import ADASYN\n",
        "from multiprocessing import Process\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.model_selection import train_test_split\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "import re\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Dkch1NUw1ET6"
      },
      "outputs": [],
      "source": [
        "root = \"\"\n",
        "data_path = root + 'data/'\n",
        "tfidf_path = data_path + \"tf-idf/\"\n",
        "vectors_path = data_path + \"vectors/\"\n",
        "classifications_path = data_path + \"classifications/\"\n",
        "results_path = data_path + \"results/\"\n",
        "\n",
        "df = pd.read_csv(data_path+\"AI_Human_cleaned.csv\", usecols = [\"text\",\"prompt_name\",\"source\",\"label\"])\n",
        "\n",
        "#df = df.sample(n=1000, random_state=42)\n",
        "\n",
        "train_df, test_df = train_test_split(df, test_size=0.25, random_state=42)\n",
        "\n",
        "folder_path = data_path\n",
        "if not os.path.exists(folder_path):\n",
        "    os.makedirs(folder_path)\n",
        "\n",
        "train_df.to_csv(data_path+'train_dataset.csv',index=False)\n",
        "train_df.to_hdf(data_path+'train_dataset.h5', key='train_dataset', mode='w')\n",
        "\n",
        "test_df.to_csv(data_path+'test_dataset.csv',index=False)\n",
        "test_df.to_hdf(data_path+'test_dataset.h5', key='test_dataset', mode='w')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9iwEumS-QFTF",
        "outputId": "21323ec8-f676-44bb-bb52-6023fd4c5bda"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\mehke\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "['i',\n",
              " 'me',\n",
              " 'my',\n",
              " 'myself',\n",
              " 'we',\n",
              " 'our',\n",
              " 'ours',\n",
              " 'ourselves',\n",
              " 'you',\n",
              " \"you're\",\n",
              " \"you've\",\n",
              " \"you'll\",\n",
              " \"you'd\",\n",
              " 'your',\n",
              " 'yours',\n",
              " 'yourself',\n",
              " 'yourselves',\n",
              " 'he',\n",
              " 'him',\n",
              " 'his',\n",
              " 'himself',\n",
              " 'she',\n",
              " \"she's\",\n",
              " 'her',\n",
              " 'hers',\n",
              " 'herself',\n",
              " 'it',\n",
              " \"it's\",\n",
              " 'its',\n",
              " 'itself',\n",
              " 'they',\n",
              " 'them',\n",
              " 'their',\n",
              " 'theirs',\n",
              " 'themselves',\n",
              " 'what',\n",
              " 'which',\n",
              " 'who',\n",
              " 'whom',\n",
              " 'this',\n",
              " 'that',\n",
              " \"that'll\",\n",
              " 'these',\n",
              " 'those',\n",
              " 'am',\n",
              " 'is',\n",
              " 'are',\n",
              " 'was',\n",
              " 'were',\n",
              " 'be',\n",
              " 'been',\n",
              " 'being',\n",
              " 'have',\n",
              " 'has',\n",
              " 'had',\n",
              " 'having',\n",
              " 'do',\n",
              " 'does',\n",
              " 'did',\n",
              " 'doing',\n",
              " 'a',\n",
              " 'an',\n",
              " 'the',\n",
              " 'and',\n",
              " 'but',\n",
              " 'if',\n",
              " 'or',\n",
              " 'because',\n",
              " 'as',\n",
              " 'until',\n",
              " 'while',\n",
              " 'of',\n",
              " 'at',\n",
              " 'by',\n",
              " 'for',\n",
              " 'with',\n",
              " 'about',\n",
              " 'against',\n",
              " 'between',\n",
              " 'into',\n",
              " 'through',\n",
              " 'during',\n",
              " 'before',\n",
              " 'after',\n",
              " 'above',\n",
              " 'below',\n",
              " 'to',\n",
              " 'from',\n",
              " 'up',\n",
              " 'down',\n",
              " 'in',\n",
              " 'out',\n",
              " 'on',\n",
              " 'off',\n",
              " 'over',\n",
              " 'under',\n",
              " 'again',\n",
              " 'further',\n",
              " 'then',\n",
              " 'once',\n",
              " 'here',\n",
              " 'there',\n",
              " 'when',\n",
              " 'where',\n",
              " 'why',\n",
              " 'how',\n",
              " 'all',\n",
              " 'any',\n",
              " 'both',\n",
              " 'each',\n",
              " 'few',\n",
              " 'more',\n",
              " 'most',\n",
              " 'other',\n",
              " 'some',\n",
              " 'such',\n",
              " 'no',\n",
              " 'nor',\n",
              " 'not',\n",
              " 'only',\n",
              " 'own',\n",
              " 'same',\n",
              " 'so',\n",
              " 'than',\n",
              " 'too',\n",
              " 'very',\n",
              " 's',\n",
              " 't',\n",
              " 'can',\n",
              " 'will',\n",
              " 'just',\n",
              " 'don',\n",
              " \"don't\",\n",
              " 'should',\n",
              " \"should've\",\n",
              " 'now',\n",
              " 'd',\n",
              " 'll',\n",
              " 'm',\n",
              " 'o',\n",
              " 're',\n",
              " 've',\n",
              " 'y',\n",
              " 'ain',\n",
              " 'aren',\n",
              " \"aren't\",\n",
              " 'couldn',\n",
              " \"couldn't\",\n",
              " 'didn',\n",
              " \"didn't\",\n",
              " 'doesn',\n",
              " \"doesn't\",\n",
              " 'hadn',\n",
              " \"hadn't\",\n",
              " 'hasn',\n",
              " \"hasn't\",\n",
              " 'haven',\n",
              " \"haven't\",\n",
              " 'isn',\n",
              " \"isn't\",\n",
              " 'ma',\n",
              " 'mightn',\n",
              " \"mightn't\",\n",
              " 'mustn',\n",
              " \"mustn't\",\n",
              " 'needn',\n",
              " \"needn't\",\n",
              " 'shan',\n",
              " \"shan't\",\n",
              " 'shouldn',\n",
              " \"shouldn't\",\n",
              " 'wasn',\n",
              " \"wasn't\",\n",
              " 'weren',\n",
              " \"weren't\",\n",
              " 'won',\n",
              " \"won't\",\n",
              " 'wouldn',\n",
              " \"wouldn't\",\n",
              " 'I',\n",
              " 'Me',\n",
              " 'My',\n",
              " 'Myself',\n",
              " 'We',\n",
              " 'Our',\n",
              " 'Ours',\n",
              " 'Ourselves',\n",
              " 'You',\n",
              " \"You're\",\n",
              " \"You've\",\n",
              " \"You'll\",\n",
              " \"You'd\",\n",
              " 'Your',\n",
              " 'Yours',\n",
              " 'Yourself',\n",
              " 'Yourselves',\n",
              " 'He',\n",
              " 'Him',\n",
              " 'His',\n",
              " 'Himself',\n",
              " 'She',\n",
              " \"She's\",\n",
              " 'Her',\n",
              " 'Hers',\n",
              " 'Herself',\n",
              " 'It',\n",
              " \"It's\",\n",
              " 'Its',\n",
              " 'Itself',\n",
              " 'They',\n",
              " 'Them',\n",
              " 'Their',\n",
              " 'Theirs',\n",
              " 'Themselves',\n",
              " 'What',\n",
              " 'Which',\n",
              " 'Who',\n",
              " 'Whom',\n",
              " 'This',\n",
              " 'That',\n",
              " \"That'll\",\n",
              " 'These',\n",
              " 'Those',\n",
              " 'Am',\n",
              " 'Is',\n",
              " 'Are',\n",
              " 'Was',\n",
              " 'Were',\n",
              " 'Be',\n",
              " 'Been',\n",
              " 'Being',\n",
              " 'Have',\n",
              " 'Has',\n",
              " 'Had',\n",
              " 'Having',\n",
              " 'Do',\n",
              " 'Does',\n",
              " 'Did',\n",
              " 'Doing',\n",
              " 'A',\n",
              " 'An',\n",
              " 'The',\n",
              " 'And',\n",
              " 'But',\n",
              " 'If',\n",
              " 'Or',\n",
              " 'Because',\n",
              " 'As',\n",
              " 'Until',\n",
              " 'While',\n",
              " 'Of',\n",
              " 'At',\n",
              " 'By',\n",
              " 'For',\n",
              " 'With',\n",
              " 'About',\n",
              " 'Against',\n",
              " 'Between',\n",
              " 'Into',\n",
              " 'Through',\n",
              " 'During',\n",
              " 'Before',\n",
              " 'After',\n",
              " 'Above',\n",
              " 'Below',\n",
              " 'To',\n",
              " 'From',\n",
              " 'Up',\n",
              " 'Down',\n",
              " 'In',\n",
              " 'Out',\n",
              " 'On',\n",
              " 'Off',\n",
              " 'Over',\n",
              " 'Under',\n",
              " 'Again',\n",
              " 'Further',\n",
              " 'Then',\n",
              " 'Once',\n",
              " 'Here',\n",
              " 'There',\n",
              " 'When',\n",
              " 'Where',\n",
              " 'Why',\n",
              " 'How',\n",
              " 'All',\n",
              " 'Any',\n",
              " 'Both',\n",
              " 'Each',\n",
              " 'Few',\n",
              " 'More',\n",
              " 'Most',\n",
              " 'Other',\n",
              " 'Some',\n",
              " 'Such',\n",
              " 'No',\n",
              " 'Nor',\n",
              " 'Not',\n",
              " 'Only',\n",
              " 'Own',\n",
              " 'Same',\n",
              " 'So',\n",
              " 'Than',\n",
              " 'Too',\n",
              " 'Very',\n",
              " 'S',\n",
              " 'T',\n",
              " 'Can',\n",
              " 'Will',\n",
              " 'Just',\n",
              " 'Don',\n",
              " \"Don't\",\n",
              " 'Should',\n",
              " \"Should've\",\n",
              " 'Now',\n",
              " 'D',\n",
              " 'Ll',\n",
              " 'M',\n",
              " 'O',\n",
              " 'Re',\n",
              " 'Ve',\n",
              " 'Y',\n",
              " 'Ain',\n",
              " 'Aren',\n",
              " \"Aren't\",\n",
              " 'Couldn',\n",
              " \"Couldn't\",\n",
              " 'Didn',\n",
              " \"Didn't\",\n",
              " 'Doesn',\n",
              " \"Doesn't\",\n",
              " 'Hadn',\n",
              " \"Hadn't\",\n",
              " 'Hasn',\n",
              " \"Hasn't\",\n",
              " 'Haven',\n",
              " \"Haven't\",\n",
              " 'Isn',\n",
              " \"Isn't\",\n",
              " 'Ma',\n",
              " 'Mightn',\n",
              " \"Mightn't\",\n",
              " 'Mustn',\n",
              " \"Mustn't\",\n",
              " 'Needn',\n",
              " \"Needn't\",\n",
              " 'Shan',\n",
              " \"Shan't\",\n",
              " 'Shouldn',\n",
              " \"Shouldn't\",\n",
              " 'Wasn',\n",
              " \"Wasn't\",\n",
              " 'Weren',\n",
              " \"Weren't\",\n",
              " 'Won',\n",
              " \"Won't\",\n",
              " 'Wouldn',\n",
              " \"Wouldn't\"]"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "SPECIAL_CHARCATERS_REMOVAL=r\"\\b\\w+\\b\"\n",
        "TOKEN_PATTERN=\"[^ \\n]+\"\n",
        "\n",
        "# Stop Words\n",
        "nltk.download('stopwords')\n",
        "STOP_WORDS = stopwords.words('english')\n",
        "STOP_WORDS += [word.capitalize() for word in STOP_WORDS]\n",
        "\n",
        "STOP_WORDS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "U76bq9Yq1PlZ"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Abbreviations\n",
        "ABBREVIATIONS = pd.read_excel(data_path+'abbreviations_eng.xls')\n",
        "ABBREVIATIONS['abbr'] = ABBREVIATIONS['abbr'].astype(str)\n",
        "ABBREVIATIONS_lowercased = ABBREVIATIONS.copy()\n",
        "ABBREVIATIONS_lowercased['abbr'] = ABBREVIATIONS_lowercased['abbr'].str.lower()\n",
        "ABBREVIATIONS_lowercased['long'] = ABBREVIATIONS_lowercased['long'].str.lower()\n",
        "ABBREVIATIONS = pd.concat([ABBREVIATIONS, ABBREVIATIONS_lowercased], ignore_index=True)\n",
        "ABBR_PATTERN = r'\\b(?:' + '|'.join(map(re.escape, ABBREVIATIONS['abbr'])) + r')\\b'\n",
        "def expand_abbreviations(text):\n",
        "    def replace(match):\n",
        "        return ABBREVIATIONS.loc[ABBREVIATIONS['abbr'] == match.group(0), 'long'].iloc[0]\n",
        "\n",
        "    return re.sub(ABBR_PATTERN, replace, text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "onWW4qVl1W_F"
      },
      "outputs": [],
      "source": [
        "def read_texts(train_or_test):\n",
        "  \"\"\"\n",
        "  reads the data from the train tweets sheet\n",
        "  :return: the data as pandas data set\n",
        "  \"\"\"\n",
        "\n",
        "  store = pd.HDFStore(data_path+f'{train_or_test}_dataset.h5')\n",
        "  df = store.select(f'{train_or_test}_dataset')\n",
        "  store.close()\n",
        "\n",
        "  columns_to_include = [\"text\"]\n",
        "  df = df[columns_to_include]\n",
        "  df.dropna(subset=[\"text\"], inplace=True)\n",
        "  return df\n",
        "\n",
        "def read_classifications(train_or_test=\"train\", classification = \"label\"):\n",
        "  \"\"\"\n",
        "  reads the data from the train tweets sheet\n",
        "  :return: the data as pandas data set\n",
        "  \"\"\"\n",
        "\n",
        "  store = pd.HDFStore(data_path+f'{train_or_test}_dataset.h5')\n",
        "  df = store.select(f'{train_or_test}_dataset')\n",
        "  store.close()\n",
        "\n",
        "  columns_to_include = [classification]\n",
        "  df = df[columns_to_include]\n",
        "  df.dropna(subset=[classification], inplace=True)\n",
        "  return df\n",
        "\n",
        "\n",
        "def get_terms():\n",
        "  columns= [\"nt\", \"f\", \"tf\",\"idf\",\"tfidf\"]\n",
        "  meanings= [\"Number of different documents that the word appears in.\",\n",
        "   \"Number of appearances of the word in all documents.\",\n",
        "   \"Term frequency.\",\n",
        "   \"Inverse document frequency.\",\n",
        "   \"Term fruquency multiplied by inverse document frequency.\"]\n",
        "  return columns, meanings\n",
        "\n",
        "\n",
        "def write_tf_idf_chart(preprocessing,path = tfidf_path):\n",
        "  train_text = read_texts(\"train\")\n",
        "  vectorizer = VECTORIZERS[preprocessing]\n",
        "  if \"L\" in preprocessing and \"O\" in preprocessing:\n",
        "      train_text[\"text\"] = \"LowerCode \" + train_text[\"text\"]\n",
        "  elif \"L\" in preprocessing:\n",
        "      train_text[\"text\"] = train_text[\"text\"].str.lower()\n",
        "\n",
        "\n",
        "  sparse_matrix = vectorizer.fit_transform(train_text[\"text\"])\n",
        "  dense_matrix = sparse_matrix.toarray()\n",
        "  sig_f = sparse_matrix.sum()\n",
        "\n",
        "  print(f\"Me: sig_f is:{sig_f}\")\n",
        "\n",
        "  # Get the vocabulary from the vectorizer object\n",
        "  vocab = vectorizer.get_feature_names_out()\n",
        "  df = pd.DataFrame(columns=['word', 'nt', 'f', 'tf', 'idf', 'tf-idf'])\n",
        "\n",
        "\n",
        "  print(f\"calculating the tfidf table for {preprocessing}:\")\n",
        "  for word , index in tqdm(vectorizer.vocabulary_.items()):\n",
        "      # print(word)\n",
        "      # The column of tf-idf values of the specific word, as the rows are the tweets\n",
        "      word_column=dense_matrix[:, index]\n",
        "      # Count the number of texts the word appears in\n",
        "      nt = np.count_nonzero(word_column)\n",
        "      # Count the number of times the word is used in all tweets\n",
        "      f = np.sum(word_column)\n",
        "      # term frequency in a document compared to the number of terms in the corpus\n",
        "      tf = f / sig_f\n",
        "      # shape[0] is the number of rows, aka the number of texts\n",
        "      n = dense_matrix.shape[0]\n",
        "      # Calculate the idf value\n",
        "      idf =np. log(n / (nt + 1))\n",
        "      # Calculate the tf-idf value\n",
        "      tf_idf = tf * idf\n",
        "      # append all the data we collected so far to each word to the data frame\n",
        "      new_df = pd.DataFrame({'word': word, 'nt': nt, 'f': f, 'tf': tf, 'idf': idf, 'tf-idf': tf_idf}, index=[0])\n",
        "\n",
        "      df = pd.concat([df, new_df], ignore_index=True)\n",
        "  print(\"done!\")\n",
        "  df = df.sort_values('tf-idf', ascending=False)\n",
        "\n",
        "  df.to_csv(path+f\"tfidf_table_{preprocessing}.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "Gv_S6ms41k3n"
      },
      "outputs": [],
      "source": [
        "NT = 1  # the minimum nuber of tweets a word in tf idf should show\n",
        "NUMBER_OF_WORDS = [1000,2000,3000,4000,5000]  # the number of the highest tfidf words\n",
        "VECTORIZERS={\n",
        "    # the tf-idf vectorizers by preprocessing method\n",
        "    #  'N' : CountVectorizer(min_df=NT,lowercase=False, token_pattern=TOKEN_PATTERN),#None, baseline\n",
        "\n",
        "    #  'S':CountVectorizer(min_df=NT,lowercase=False, token_pattern=TOKEN_PATTERN,stop_words=STOP_WORDS),#Stop word removal\n",
        "    #  'C':CountVectorizer(min_df=NT,lowercase=False, token_pattern=SPECIAL_CHARCATERS_REMOVAL),#special Characters removal\n",
        "    #  'O':CountVectorizer(min_df=NT,lowercase=False, token_pattern=TOKEN_PATTERN, preprocessor=expand_abbreviations),#Open abbreviations\n",
        "    # # '3gram' : CountVectorizer(min_df=NT,lowercase=False, analyzer='char', ngram_range=(3, 3)),\n",
        "    # # '4gram' : CountVectorizer(min_df=NT,lowercase=False, analyzer='char', ngram_range=(4, 4)),\n",
        "     'L' : CountVectorizer(min_df=NT,lowercase=True, token_pattern=TOKEN_PATTERN),# Lowercase\n",
        "\n",
        "    # # pairing preprocssing methods\n",
        "    'SC':CountVectorizer(min_df=NT,lowercase=False, token_pattern=SPECIAL_CHARCATERS_REMOVAL,stop_words=STOP_WORDS),\n",
        "    # 'SO':CountVectorizer(min_df=NT,lowercase=False, token_pattern=TOKEN_PATTERN,stop_words=STOP_WORDS,preprocessor=expand_abbreviations),\n",
        "    'SL':CountVectorizer(min_df=NT,lowercase=True, token_pattern=TOKEN_PATTERN,stop_words=STOP_WORDS),\n",
        "    # 'CO':CountVectorizer(min_df=NT,lowercase=False, token_pattern=SPECIAL_CHARCATERS_REMOVAL,preprocessor=expand_abbreviations),\n",
        "    'CL':CountVectorizer(min_df=NT,lowercase=True, token_pattern=SPECIAL_CHARCATERS_REMOVAL),\n",
        "    # 'OL':CountVectorizer(min_df=NT,lowercase=True, token_pattern=TOKEN_PATTERN,preprocessor=expand_abbreviations),\n",
        "\n",
        "    # # Trio preprocessing methods\n",
        "    # 'SCO':CountVectorizer(min_df=NT,lowercase=False, token_pattern=SPECIAL_CHARCATERS_REMOVAL,stop_words=STOP_WORDS,preprocessor=expand_abbreviations),\n",
        "    # 'COL':CountVectorizer(min_df=NT,lowercase=True, token_pattern=SPECIAL_CHARCATERS_REMOVAL,preprocessor=expand_abbreviations),\n",
        "    # 'SOL':CountVectorizer(min_df=NT,lowercase=True, token_pattern=TOKEN_PATTERN,stop_words=STOP_WORDS, preprocessor=expand_abbreviations),\n",
        "    'SCL':CountVectorizer(min_df=NT,lowercase=True, token_pattern=SPECIAL_CHARCATERS_REMOVAL,stop_words=STOP_WORDS),\n",
        "\n",
        "    # # # All preprocessing\n",
        "    # 'SCOL':CountVectorizer(min_df=NT,lowercase=True, token_pattern=SPECIAL_CHARCATERS_REMOVAL,stop_words=STOP_WORDS,preprocessor=expand_abbreviations)\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "P_M7v3Wc1nqq",
        "outputId": "5db04750-ecb2-4709-a145-6248736e1528"
      },
      "outputs": [],
      "source": [
        "\n",
        "#itirate over each preprocessing method and writes the tfidf chart\n",
        "for preprocessing, vectorizer in VECTORIZERS.items():\n",
        "  print(\"------------------------------------------\")\n",
        "  print(f\"writing the {preprocessing} tf-idf chart:\")\n",
        "  write_tf_idf_chart(preprocessing)\n",
        "  print(\"------------------------------------------\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "EpTqOuwZ15se"
      },
      "outputs": [],
      "source": [
        "CLASSIFY = ['label']\n",
        "\n",
        "def write_classification(text, classification, set):\n",
        "    classified_text_df = pd.DataFrame(columns=[\"class: \"+classification])\n",
        "    if classification == 'label':\n",
        "        classified_text_df[\"class: \"+classification] = text[\"label\"].apply(lambda x: 0 if x == 0 else 1)\n",
        "    folder_path = 'classification/'\n",
        "    if not os.path.exists(folder_path):\n",
        "        os.makedirs(folder_path)\n",
        "    classified_text_df.to_csv(classifications_path+f'{set}_classification_{classification}.csv', index=False)    \n",
        "    classified_text_df.to_hdf(classifications_path+f'{set}_classification_{classification}.h5', key='classification', mode='w')\n",
        "\n",
        "def write_vectors(tfidf_vocabulary,vectors,set,amount,preprocessing):\n",
        "  dense_vectors = vectors.toarray()\n",
        "  df = pd.DataFrame(dense_vectors, columns=tfidf_vocabulary)\n",
        "  folder_path = 'vectors/'\n",
        "  if not os.path.exists(folder_path):\n",
        "        os.makedirs(folder_path)\n",
        "  df.to_csv(f'vectors/{preprocessing}_{set}_vectors_{amount}.csv',index=False)\n",
        "  df.to_hdf(f'vectors/{preprocessing}_{set}_vectors_{amount}.h5', key='vectors', mode='w')\n",
        "\n",
        "def get_words(amount,preprocessing):\n",
        "  words = pd.read_csv(tfidf_path+f'tfidf_table_{preprocessing}.csv')\n",
        "  words = words.head(amount)\n",
        "  words = words['word'].tolist()\n",
        "  return words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 8/8 [02:30<00:00, 18.85s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 8/8 [04:08<00:00, 31.05s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "3000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 8/8 [05:47<00:00, 43.45s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "4000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 8/8 [07:31<00:00, 56.49s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "5000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 8/8 [09:48<00:00, 73.51s/it]\n"
          ]
        }
      ],
      "source": [
        "train_texts = read_texts(\"train\")\n",
        "test_texts = read_texts(\"test\")\n",
        "for amount in NUMBER_OF_WORDS:\n",
        "  print(amount)\n",
        "  VECTORIZERS={ # the tf-idf vectorizers by preprocessing method\n",
        "    'N' : TfidfVectorizer(vocabulary=get_words(amount,'N'),min_df=NT,lowercase=False, token_pattern=TOKEN_PATTERN),#None, baseline\n",
        "    \n",
        "    'S':TfidfVectorizer(vocabulary=get_words(amount,'S'),min_df=NT,lowercase=False, token_pattern=TOKEN_PATTERN,stop_words=STOP_WORDS),#Stop word removal\n",
        "    'C':TfidfVectorizer(vocabulary=get_words(amount,'C'),min_df=NT,lowercase=False, token_pattern=SPECIAL_CHARCATERS_REMOVAL),#special Characters removal\n",
        "    # 'O':TfidfVectorizer(vocabulary=get_words(amount,'O'),min_df=NT,lowercase=False, token_pattern=TOKEN_PATTERN, preprocessor=expand_abbreviations),#Open abbreviations\n",
        "    'L' : TfidfVectorizer(vocabulary=get_words(amount,'L'),min_df=NT,lowercase=True, token_pattern=TOKEN_PATTERN),# Lowercase\n",
        "    \n",
        "    # pairing preprocssing methods\n",
        "    'SC':TfidfVectorizer(vocabulary=get_words(amount,'SC'),min_df=NT,lowercase=False, token_pattern=SPECIAL_CHARCATERS_REMOVAL,stop_words=STOP_WORDS),\n",
        "    # 'SO':TfidfVectorizer(vocabulary=get_words(amount,'SO'),min_df=NT,lowercase=False, token_pattern=TOKEN_PATTERN,stop_words=STOP_WORDS,preprocessor=expand_abbreviations),\n",
        "    'SL':TfidfVectorizer(vocabulary=get_words(amount,'SL'),min_df=NT,lowercase=True, token_pattern=TOKEN_PATTERN,stop_words=STOP_WORDS),\n",
        "    # 'CO':TfidfVectorizer(vocabulary=get_words(amount,'CO'),min_df=NT,lowercase=False, token_pattern=SPECIAL_CHARCATERS_REMOVAL,preprocessor=expand_abbreviations),\n",
        "    'CL':TfidfVectorizer(vocabulary=get_words(amount,'CL'),min_df=NT,lowercase=True, token_pattern=SPECIAL_CHARCATERS_REMOVAL),\n",
        "    # 'OL':TfidfVectorizer(vocabulary=get_words(amount,'OL'),min_df=NT,lowercase=True, token_pattern=TOKEN_PATTERN,preprocessor=expand_abbreviations),\n",
        "    \n",
        "    # Trio preprocessing methods\n",
        "    # 'SCO':TfidfVectorizer(vocabulary=get_words(amount,'SCO'),min_df=NT,lowercase=False, token_pattern=SPECIAL_CHARCATERS_REMOVAL,stop_words=STOP_WORDS,preprocessor=expand_abbreviations),\n",
        "    # 'COL':TfidfVectorizer(vocabulary=get_words(amount,'COL'),min_df=NT,lowercase=True, token_pattern=SPECIAL_CHARCATERS_REMOVAL,preprocessor=expand_abbreviations),\n",
        "    # 'SOL':TfidfVectorizer(vocabulary=get_words(amount,'SOL'),min_df=NT,lowercase=True, token_pattern=TOKEN_PATTERN,stop_words=STOP_WORDS, preprocessor=expand_abbreviations),\n",
        "    'SCL':TfidfVectorizer(vocabulary=get_words(amount,'SCL'),min_df=NT,lowercase=True, token_pattern=SPECIAL_CHARCATERS_REMOVAL,stop_words=STOP_WORDS),\n",
        "    \n",
        "    # All preprocessing\n",
        "    # 'SCOL':TfidfVectorizer(vocabulary=get_words(amount,'SCOL'),min_df=NT,lowercase=True, token_pattern=SPECIAL_CHARCATERS_REMOVAL,stop_words=STOP_WORDS,preprocessor=expand_abbreviations)\n",
        "}\n",
        "  for preprocessing, vectorizer in tqdm(VECTORIZERS.items()):\n",
        "    tfidf_vocabulary=get_words(amount,preprocessing)\n",
        "    train_vectors = vectorizer.fit_transform(train_texts[\"text\"])\n",
        "    write_vectors(tfidf_vocabulary,train_vectors,\"train\",amount,preprocessing)\n",
        "\n",
        "    test_vectors = vectorizer.fit_transform(test_texts[\"text\"])\n",
        "    write_vectors(tfidf_vocabulary,test_vectors,\"test\",amount,preprocessing)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "LABELS = df['label'].to_numpy()\n",
        "\n",
        "OVERSAMPLERS = {\n",
        "    'ROS': RandomOverSampler(random_state=42),\n",
        "    'SMOTE': SMOTE(random_state=42),\n",
        "    'ADASYN': ADASYN(random_state=42),\n",
        "    'NONE': None\n",
        "}\n",
        "\n",
        "ML_CLASSIFIERS = {\n",
        "    'RandomForest': RandomForestClassifier(random_state=42),\n",
        "    'SVC': SVC(random_state=42),\n",
        "    'AdaBoost': AdaBoostClassifier(random_state=42),\n",
        "    'DecisionTree': DecisionTreeClassifier(random_state=42),\n",
        "    'MLP': MLPClassifier(random_state=42),\n",
        "    'MultinomialNB': MultinomialNB(),\n",
        "    'LogisticRegression': LogisticRegression(random_state=42)\n",
        "}\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_vectors(set,amount,preprocessing):\n",
        "  vectors = pd.read_hdf(vectors_path+f'{preprocessing}_{set}_vectors_{amount}.h5')\n",
        "  return vectors\n",
        "\n",
        "\n",
        "\n",
        "def oversample(oversampler, amount, preprocessing):\n",
        "    train_vectors = load_vectors(\"train\",amount,preprocessing)\n",
        "    train_labels = read_classifications(\"train\")\n",
        "\n",
        "    oversampler = OVERSAMPLERS[oversampler]\n",
        "\n",
        "    if oversampler is not None:\n",
        "        train_vectors, train_labels = oversampler.fit_resample(train_vectors, train_labels)\n",
        "\n",
        "    return train_vectors, train_labels\n",
        "\n",
        "def classify(x_train, y_train, x_test, y_test, classifier):\n",
        "    classifier = ML_CLASSIFIERS[classifier]\n",
        "    classifier.fit(x_train, y_train)\n",
        "    y_pred = classifier.predict(x_test)\n",
        "    return y_pred\n",
        "\n",
        "def evaluate(y_test, y_pred):\n",
        "    f1 = f1_score(y_test, y_pred)\n",
        "    precision = precision_score(y_test, y_pred)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    recall = recall_score(y_test, y_pred)\n",
        "    roc_auc = roc_auc_score(y_test, y_pred)\n",
        "    results = {\n",
        "        'f1': f1,\n",
        "        'precision': precision,\n",
        "        'accuracy': accuracy,\n",
        "        'recall': recall,\n",
        "        'roc_auc': roc_auc\n",
        "    }\n",
        "\n",
        "    return results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "amount: 1000, preprocessing: L, oversampler: ROS, classifier: RandomForest\n"
          ]
        },
        {
          "ename": "FileNotFoundError",
          "evalue": "File data/vectors/L_train_vectors_1000.h5 does not exist",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[16], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m classifier \u001b[38;5;129;01min\u001b[39;00m ML_CLASSIFIERS\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m      5\u001b[0m   \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mamount: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mamount\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, preprocessing: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpreprocessing\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, oversampler: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moversampler\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, classifier: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclassifier\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 6\u001b[0m   train_vectors, train_labels \u001b[38;5;241m=\u001b[39m \u001b[43moversample\u001b[49m\u001b[43m(\u001b[49m\u001b[43moversampler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mamount\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreprocessing\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m   test_vectors \u001b[38;5;241m=\u001b[39m load_vectors(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m,amount,preprocessing)\n\u001b[0;32m      8\u001b[0m   test_labels \u001b[38;5;241m=\u001b[39m read_classifications(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "Cell \u001b[1;32mIn[10], line 8\u001b[0m, in \u001b[0;36moversample\u001b[1;34m(oversampler, amount, preprocessing)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moversample\u001b[39m(oversampler, amount, preprocessing):\n\u001b[1;32m----> 8\u001b[0m     train_vectors \u001b[38;5;241m=\u001b[39m \u001b[43mload_vectors\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mamount\u001b[49m\u001b[43m,\u001b[49m\u001b[43mpreprocessing\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m     train_labels \u001b[38;5;241m=\u001b[39m read_classifications(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "Cell \u001b[1;32mIn[10], line 2\u001b[0m, in \u001b[0;36mload_vectors\u001b[1;34m(set, amount, preprocessing)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_vectors\u001b[39m(\u001b[38;5;28mset\u001b[39m,amount,preprocessing):\n\u001b[1;32m----> 2\u001b[0m   vectors \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_hdf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvectors_path\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mpreprocessing\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mset\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_vectors_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mamount\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.h5\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m vectors\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\pandas\\io\\pytables.py:424\u001b[0m, in \u001b[0;36mread_hdf\u001b[1;34m(path_or_buf, key, mode, errors, where, start, stop, columns, iterator, chunksize, **kwargs)\u001b[0m\n\u001b[0;32m    421\u001b[0m     exists \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    423\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m exists:\n\u001b[1;32m--> 424\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFile \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_buf\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not exist\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    426\u001b[0m store \u001b[38;5;241m=\u001b[39m HDFStore(path_or_buf, mode\u001b[38;5;241m=\u001b[39mmode, errors\u001b[38;5;241m=\u001b[39merrors, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    427\u001b[0m \u001b[38;5;66;03m# can't auto open/close if we are using an iterator\u001b[39;00m\n\u001b[0;32m    428\u001b[0m \u001b[38;5;66;03m# so delegate to the iterator\u001b[39;00m\n",
            "\u001b[1;31mFileNotFoundError\u001b[0m: File data/vectors/L_train_vectors_1000.h5 does not exist"
          ]
        }
      ],
      "source": [
        "for amount in NUMBER_OF_WORDS:\n",
        "  for preprocessing in VECTORIZERS.keys():\n",
        "    for oversampler in OVERSAMPLERS.keys():\n",
        "      for classifier in ML_CLASSIFIERS.keys():\n",
        "        print(f\"amount: {amount}, preprocessing: {preprocessing}, oversampler: {oversampler}, classifier: {classifier}\")\n",
        "        train_vectors, train_labels = oversample(oversampler, amount, preprocessing)\n",
        "        test_vectors = load_vectors(\"test\",amount,preprocessing)\n",
        "        test_labels = read_classifications(\"test\")\n",
        "\n",
        "        y_pred = classify(train_vectors, train_labels, test_vectors, test_labels, classifier)\n",
        "        results = evaluate(test_labels, y_pred)\n",
        "\n",
        "        results_df = pd.DataFrame(results, index=[0])\n",
        "        results_df.to_csv(results_path+f'results_{amount}_{preprocessing}_{oversampler}_{classifier}.csv', index=False)\n",
        "        results_df.to_hdf(results_path+f'results_{amount}_{preprocessing}_{oversampler}_{classifier}.h5', key='results', mode='w')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyOere4mPPYtV9Bk+zL1/IC4",
      "include_colab_link": true,
      "mount_file_id": "1DGfDuUDU6wW8aTLgco5ex3zYSCEh39ZL",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
